{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN for Cifar-10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "befor reformat train shape:  (50000, 3072)\n",
      "after reformat train shape:  (50000, 32, 32, 3)\n",
      "Initialized\n",
      "loss at epoch 0:\n",
      "cost: 2.2278483298110894, accuracy: 0.3409999999999969\n",
      "loss at epoch 1:\n",
      "cost: 1.773177111053468, accuracy: 0.4282999999999944\n",
      "loss at epoch 2:\n",
      "cost: 1.7331751992797846, accuracy: 0.44525999999999505\n",
      "loss at epoch 3:\n",
      "cost: 1.7127022282791162, accuracy: 0.4557999999999941\n",
      "loss at epoch 4:\n",
      "cost: 1.6956869562911976, accuracy: 0.4662799999999943\n",
      "loss at epoch 5:\n",
      "cost: 1.681262268142698, accuracy: 0.4733399999999935\n",
      "loss at epoch 6:\n",
      "cost: 1.6714608768272368, accuracy: 0.4776399999999941\n",
      "loss at epoch 7:\n",
      "cost: 1.6660958282279934, accuracy: 0.48061999999999294\n",
      "loss at epoch 8:\n",
      "cost: 1.6620354361915612, accuracy: 0.48183999999999305\n",
      "loss at epoch 9:\n",
      "cost: 1.6580306155013993, accuracy: 0.4844799999999936\n",
      "Testing......\n",
      "Test accuracy: 0.47839999198913574\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "def unpickle(file):\n",
    "    with open(file, 'rb') as fo:\n",
    "        d = pickle.load(fo, encoding='latin1')\n",
    "    return d\n",
    "    \n",
    "def onehot(labels):\n",
    "    ''' one-hot encoding'''\n",
    "    n_sample = len(labels)\n",
    "    n_class = max(labels) + 1\n",
    "    onehot_labels = np.zeros((n_sample, n_class))\n",
    "    onehot_labels[np.arange(n_sample), labels] = 1\n",
    "\n",
    "    return onehot_labels\n",
    "\n",
    "def reformat(dataset):\n",
    "    image_size=np.sqrt(dataset.shape[1]/3)\n",
    "    dataset = dataset.reshape((dataset.shape[0], image_size.astype(int), image_size.astype(int), num_channels)).astype(np.float32)\n",
    "    #labels = (np.arange(num_labels) == labels[:,None]).astype(np.float32)\n",
    "  \n",
    "    return dataset\n",
    "\n",
    "data1 = unpickle('cifar-10-batches-py/data_batch_1')\n",
    "data2 = unpickle('cifar-10-batches-py/data_batch_2')\n",
    "data3 = unpickle('cifar-10-batches-py/data_batch_3')\n",
    "data4 = unpickle('cifar-10-batches-py/data_batch_4')\n",
    "data5 = unpickle('cifar-10-batches-py/data_batch_5')\n",
    "\n",
    "train_dataset = np.concatenate((data1['data'], data2['data'], data3['data'], data4['data'], data5['data']), axis=0)\n",
    "label = np.concatenate((data1['labels'], data2['labels'], data3['labels'], data4['labels'], data5['labels']), axis=0)\n",
    "train_labels = onehot(label)\n",
    "train_dataset = train_dataset / 255.0\n",
    "print (\"befor reformat train shape: \", train_dataset.shape)\n",
    "train_dataset = reformat(train_dataset)\n",
    "print (\"after reformat train shape: \", train_dataset.shape)\n",
    "\n",
    "test = unpickle('cifar-10-batches-py/test_batch')\n",
    "test_dataset = test['data'] / 255.0\n",
    "test_labels = onehot(test['labels'])\n",
    "\n",
    "test_dataset= reformat(test_dataset)\n",
    "\n",
    "n_sample = train_dataset.shape[0]\n",
    "num_channels = 3\n",
    "num_labels = 10\n",
    "batch_size = 16\n",
    "patch_size = 3\n",
    "depth = 16\n",
    "num_hidden = 64\n",
    "beta = 0.01\n",
    "image_size = 32\n",
    "total_epoch = 10\n",
    "learning_rate = 0.001\n",
    "\n",
    "x = tf.placeholder(tf.float32, [None, image_size, image_size, num_channels])\n",
    "y = tf.placeholder(tf.float32, [None, num_labels])\n",
    "\n",
    "# Parameters\n",
    "layer1_weights = tf.Variable(tf.truncated_normal([patch_size, patch_size, num_channels, depth], stddev=0.1))\n",
    "layer1_biases = tf.Variable(tf.zeros([depth]))\n",
    "layer2_weights = tf.Variable(tf.truncated_normal([patch_size, patch_size, depth, depth], stddev=0.1))\n",
    "layer2_biases = tf.Variable(tf.constant(1.0, shape=[depth]))\n",
    "layer3_weights = tf.Variable(tf.truncated_normal([image_size // 4 * image_size // 4 * depth, num_hidden], stddev=0.1))\n",
    "layer3_biases = tf.Variable(tf.constant(1.0, shape=[num_hidden]))\n",
    "layer4_weights = tf.Variable(tf.truncated_normal([num_hidden, num_labels], stddev=0.1))\n",
    "layer4_biases = tf.Variable(tf.constant(1.0, shape=[num_labels]))\n",
    "\n",
    "# Define Model\n",
    "def model(input_image):\n",
    "    conv1 = tf.nn.conv2d(input_image, layer1_weights, [1, 2, 2, 1], padding='SAME')\n",
    "    hidden1 = tf.nn.relu(conv1 + layer1_biases)\n",
    "    conv2 = tf.nn.conv2d(hidden1, layer2_weights, [1, 2, 2, 1], padding='SAME')\n",
    "    hidden2 = tf.nn.relu(conv2 + layer2_biases)\n",
    "    shape = hidden2.get_shape().as_list()\n",
    "    reshape = tf.reshape(hidden2, [-1, shape[1] * shape[2] * shape[3]])\n",
    "    hidden = tf.nn.relu(tf.matmul(reshape, layer3_weights) + layer3_biases)\n",
    "    return tf.matmul(hidden, layer4_weights) + layer4_biases\n",
    "\n",
    "# Training computation.\n",
    "logits = model(x)\n",
    "loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=y, logits=logits)\\\n",
    "                      +beta*tf.nn.l2_loss(layer1_weights)#regularization\n",
    "                      +beta*tf.nn.l2_loss(layer2_weights)\n",
    "                      +beta*tf.nn.l2_loss(layer3_weights)\n",
    "                      +beta*tf.nn.l2_loss(layer4_weights)\n",
    "                     \n",
    "                     )\n",
    "\n",
    "# Optimizer\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate).minimize(loss)\n",
    "\n",
    "prediction = tf.equal(tf.argmax(logits, 1), tf.argmax(y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(prediction, tf.float32))\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    print('Initialized')\n",
    "    for epoch in range(total_epoch):\n",
    "        # calculate total batch within a epoch\n",
    "        num_steps =  train_dataset.shape[0] // batch_size\n",
    "        avg_cost = 0 \n",
    "        avg_acc = 0.0\n",
    "        for step in range(num_steps):\n",
    "            offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "            batch_data = train_dataset[offset:(offset + batch_size), :, :, :]\n",
    "            batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "            feed_dict = { x: batch_data, y: batch_labels}\n",
    "            _, l, train_accuracy_ = sess.run([optimizer, loss, accuracy], feed_dict=feed_dict)\n",
    "            avg_cost = avg_cost + (l/num_steps)\n",
    "            avg_acc = avg_acc + (train_accuracy_/num_steps)\n",
    "        print('loss at epoch {}:\\ncost: {}, accuracy: {}'.format(epoch, avg_cost, avg_acc))\n",
    "            \n",
    "    '''\n",
    "    Testing Part\n",
    "    ''' \n",
    "    \n",
    "    print('Testing......')\n",
    "    feed_dict = { x: test_dataset, y: test_labels}\n",
    "    test_accuracy_ = sess.run(accuracy, feed_dict=feed_dict)\n",
    "    print('Test accuracy: {}'.format(test_accuracy_))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cat and Dog image classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#每一張照片大小不一樣\n",
    "import numpy as np\n",
    "from os import listdir\n",
    "from random import shuffle\n",
    "from PIL import Image                   #python讀取照片套件\n",
    "\n",
    "image_path = [\"./dataset/cat_dog/cat\", \"./dataset/cat_dog/dog\"]\n",
    "\n",
    "def preprocess_to_data_file(data_dir_list, ratio=0.8):\n",
    "\n",
    "    total_list = [] \n",
    "   \n",
    "    '''\n",
    "        create all images into (image path, label) format and write to total_data.txt \n",
    "    ''' \n",
    "    with open('total_data.txt', 'w') as f:                                                #cat:0, dog:1\n",
    "        for index, data_dir in enumerate(data_dir_list):\n",
    "            for filename in listdir(data_dir):\n",
    "                #print(\"{} {}\".format(data_dir_list[index]+'/'+filename, index))\n",
    "                f.write('{} {}\\n'.format(data_dir_list[index]+'/'+filename.replace(' ',''), index)) #不懂為何要.replace(' ','')\n",
    "                total_list.append(data_dir_list[index]+'/'+filename.replace(' ','')+' '+str(index))\n",
    "\n",
    "    #print(total_list)\n",
    "    '''\n",
    "        shuffle total_list\n",
    "    ''' \n",
    "    shuffle(total_list)\n",
    "\n",
    "    '''\n",
    "        split total_list to train_list and test_list. \n",
    "        write train_list/test_list into train_data.txt/test_data.txt\n",
    "    ''' \n",
    "    train_list = total_list[:int(ratio*len(total_list))]\n",
    "    test_list = total_list[int(ratio*len(total_list)):]\n",
    "\n",
    "    with open('train_data.txt', 'w') as f:\n",
    "        for i in train_list:\n",
    "            f.write(i+'\\n')\n",
    "\n",
    "    with open('test_data.txt', 'w') as f:\n",
    "        for i in test_list:\n",
    "            f.write(i+'\\n')\n",
    "\n",
    "preprocess_to_data_file(image_path, 0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from random import shuffle\n",
    "from PIL import Image\n",
    "import tensorflow as tf\n",
    "import os\n",
    "    \n",
    "image_size = 100\n",
    "num_labels = 2\n",
    "num_channels = 3 \n",
    "\n",
    "def load_txt(file_path):                      #擷取image path及label\n",
    "    file=open(file_path)\n",
    "    lines=file.readlines()\n",
    "    paths=[]\n",
    "    labels=[]\n",
    "    for line in lines:\n",
    "        line=line.split()\n",
    "        paths.append(line[0])\n",
    "        labels.append(int(line[1]))\n",
    "    return paths, labels\n",
    "\n",
    "def one_hot(values):\n",
    "    values = np.asarray(values)\n",
    "    n_values = np.max(values) + 1\n",
    "    return np.eye(n_values)[values]\n",
    "\n",
    "def load_batch_data(paths, labels):\n",
    "    images = []\n",
    "    for path in paths:\n",
    "        image = Image.open(path)\n",
    "        \n",
    "        #resize and normalize\n",
    "        image = image.resize((image_size, image_size))\n",
    "        image = np.asarray(image)/255.0\n",
    "        images.append(image)\n",
    "           \n",
    "    #one-hot\n",
    "    one_hot_labels = one_hot(labels)\n",
    "    return images, one_hot_labels\n",
    "\n",
    "'''\n",
    "def reformat(dataset, labels):\n",
    "    dataset = dataset.reshape((-1, image_size, image_size, num_channels)).astype(np.float32)\n",
    "    labels = (np.arange(num_labels) == labels[:,None]).astype(np.float32)\n",
    "    return dataset, labels\n",
    "'''\n",
    "\n",
    "train_data, train_labels = load_txt(\"train_data.txt\")\n",
    "test_data, test_labels = load_txt(\"train_data.txt\")\n",
    "\n",
    "#imgs, labels = load_batch_data(train_data[0:5], train_labels[0:5])\n",
    "#print (imgs[0].shape)\n",
    "#print (labels[0].shape)\n",
    "#print (train_data)\n",
    "#print (train_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "step=0, loss=32.20766067504883, accuracy=0.4375\n",
      "test accuracy = 0.5180000066757202\n",
      "step=100, loss=24.193283081054688, accuracy=0.375\n",
      "test accuracy = 0.5040000081062317\n",
      "step=200, loss=22.487510681152344, accuracy=0.3125\n",
      "test accuracy = 0.5070000290870667\n",
      "step=300, loss=20.93109703063965, accuracy=0.125\n",
      "test accuracy = 0.5210000276565552\n",
      "step=400, loss=19.77070426940918, accuracy=0.4375\n",
      "test accuracy = 0.5120000243186951\n",
      "step=500, loss=18.45924186706543, accuracy=0.4375\n",
      "test accuracy = 0.5214999914169312\n",
      "step=600, loss=17.191646575927734, accuracy=0.6875\n",
      "test accuracy = 0.5115000009536743\n",
      "step=700, loss=16.313167572021484, accuracy=0.6875\n",
      "test accuracy = 0.5404999852180481\n",
      "step=800, loss=15.501067161560059, accuracy=0.625\n",
      "test accuracy = 0.5189999938011169\n",
      "step=900, loss=14.899615287780762, accuracy=0.4375\n",
      "test accuracy = 0.5490000247955322\n",
      "step=1000, loss=14.150572776794434, accuracy=0.3125\n",
      "test accuracy = 0.5260000228881836\n",
      "step=1100, loss=14.119354248046875, accuracy=0.25\n",
      "test accuracy = 0.48899999260902405\n",
      "step=1200, loss=12.816082954406738, accuracy=0.75\n",
      "test accuracy = 0.5184999704360962\n",
      "loss at epoch 0:\n",
      "cost: 17.88569075393676, accuracy: 0.5179000000000022\n",
      "step=0, loss=12.767118453979492, accuracy=0.4375\n",
      "test accuracy = 0.5705000162124634\n",
      "step=100, loss=12.391969680786133, accuracy=0.5\n",
      "test accuracy = 0.5274999737739563\n",
      "step=200, loss=11.885543823242188, accuracy=0.5\n",
      "test accuracy = 0.5899999737739563\n",
      "step=300, loss=11.508058547973633, accuracy=0.625\n",
      "test accuracy = 0.5619999766349792\n",
      "step=400, loss=11.32984733581543, accuracy=0.4375\n",
      "test accuracy = 0.5864999890327454\n",
      "step=500, loss=10.898221969604492, accuracy=0.375\n",
      "test accuracy = 0.5910000205039978\n",
      "step=600, loss=10.467061996459961, accuracy=0.75\n",
      "test accuracy = 0.5734999775886536\n",
      "step=700, loss=10.242156982421875, accuracy=0.6875\n",
      "test accuracy = 0.6014999747276306\n",
      "step=800, loss=9.980210304260254, accuracy=0.625\n",
      "test accuracy = 0.531499981880188\n",
      "step=900, loss=9.911508560180664, accuracy=0.4375\n",
      "test accuracy = 0.6075000166893005\n",
      "step=1000, loss=9.515547752380371, accuracy=0.625\n",
      "test accuracy = 0.5945000052452087\n",
      "step=1100, loss=9.897631645202637, accuracy=0.25\n",
      "test accuracy = 0.5164999961853027\n",
      "step=1200, loss=8.958293914794922, accuracy=0.75\n",
      "test accuracy = 0.5235000252723694\n",
      "loss at epoch 1:\n",
      "cost: 10.65786870651245, accuracy: 0.5631999999999981\n",
      "step=0, loss=9.0421724319458, accuracy=0.5\n",
      "test accuracy = 0.6039999723434448\n",
      "step=100, loss=8.874662399291992, accuracy=0.5625\n",
      "test accuracy = 0.5674999952316284\n",
      "step=200, loss=8.709406852722168, accuracy=0.625\n",
      "test accuracy = 0.6039999723434448\n",
      "step=300, loss=8.46300220489502, accuracy=0.6875\n",
      "test accuracy = 0.6244999766349792\n",
      "step=400, loss=8.697029113769531, accuracy=0.4375\n",
      "test accuracy = 0.5554999709129333\n",
      "step=500, loss=8.241374969482422, accuracy=0.4375\n",
      "test accuracy = 0.6075000166893005\n",
      "step=600, loss=7.9189605712890625, accuracy=0.6875\n",
      "test accuracy = 0.5899999737739563\n",
      "step=700, loss=7.867691516876221, accuracy=0.6875\n",
      "test accuracy = 0.6225000023841858\n",
      "step=800, loss=7.6768341064453125, accuracy=0.625\n",
      "test accuracy = 0.5734999775886536\n",
      "step=900, loss=7.619296550750732, accuracy=0.5\n",
      "test accuracy = 0.6480000019073486\n",
      "step=1000, loss=7.359004020690918, accuracy=0.8125\n",
      "test accuracy = 0.6535000205039978\n",
      "step=1100, loss=7.787329196929932, accuracy=0.25\n",
      "test accuracy = 0.5734999775886536\n",
      "step=1200, loss=6.985284805297852, accuracy=0.75\n",
      "test accuracy = 0.5335000157356262\n",
      "loss at epoch 2:\n",
      "cost: 8.04956363449096, accuracy: 0.5999999999999972\n",
      "step=0, loss=7.130617141723633, accuracy=0.625\n",
      "test accuracy = 0.6480000019073486\n",
      "step=100, loss=6.978696823120117, accuracy=0.6875\n",
      "test accuracy = 0.6179999709129333\n",
      "step=200, loss=6.958833694458008, accuracy=0.625\n",
      "test accuracy = 0.609499990940094\n",
      "step=300, loss=6.72003173828125, accuracy=0.6875\n",
      "test accuracy = 0.6625000238418579\n",
      "step=400, loss=7.052342414855957, accuracy=0.4375\n",
      "test accuracy = 0.5515000224113464\n",
      "step=500, loss=6.592379570007324, accuracy=0.4375\n",
      "test accuracy = 0.593500018119812\n",
      "step=600, loss=6.311580657958984, accuracy=0.6875\n",
      "test accuracy = 0.6150000095367432\n",
      "step=700, loss=6.310542106628418, accuracy=0.6875\n",
      "test accuracy = 0.6395000219345093\n",
      "step=800, loss=6.177494525909424, accuracy=0.6875\n",
      "test accuracy = 0.5929999947547913\n",
      "step=900, loss=6.1327900886535645, accuracy=0.625\n",
      "test accuracy = 0.6610000133514404\n",
      "step=1000, loss=5.942741394042969, accuracy=0.75\n",
      "test accuracy = 0.6554999947547913\n",
      "step=1100, loss=6.224058151245117, accuracy=0.4375\n",
      "test accuracy = 0.625\n",
      "step=1200, loss=5.628150939941406, accuracy=0.875\n",
      "test accuracy = 0.5640000104904175\n",
      "loss at epoch 3:\n",
      "cost: 6.44916487541199, accuracy: 0.6233999999999984\n",
      "step=0, loss=5.82049560546875, accuracy=0.6875\n",
      "test accuracy = 0.6579999923706055\n",
      "step=100, loss=5.696538925170898, accuracy=0.6875\n",
      "test accuracy = 0.628000020980835\n",
      "step=200, loss=5.715961456298828, accuracy=0.5\n",
      "test accuracy = 0.609000027179718\n",
      "step=300, loss=5.500371932983398, accuracy=0.6875\n",
      "test accuracy = 0.659500002861023\n",
      "step=400, loss=5.590244293212891, accuracy=0.4375\n",
      "test accuracy = 0.5960000157356262\n",
      "step=500, loss=5.435653209686279, accuracy=0.5\n",
      "test accuracy = 0.6010000109672546\n",
      "step=600, loss=5.18450927734375, accuracy=0.875\n",
      "test accuracy = 0.6489999890327454\n",
      "step=700, loss=5.17411994934082, accuracy=0.6875\n",
      "test accuracy = 0.6660000085830688\n",
      "step=800, loss=5.079897403717041, accuracy=0.75\n",
      "test accuracy = 0.6520000100135803\n",
      "step=900, loss=5.060050964355469, accuracy=0.6875\n",
      "test accuracy = 0.6834999918937683\n",
      "step=1000, loss=4.899622440338135, accuracy=0.75\n",
      "test accuracy = 0.6485000252723694\n",
      "step=1100, loss=5.310598373413086, accuracy=0.25\n",
      "test accuracy = 0.6014999747276306\n",
      "step=1200, loss=4.663755893707275, accuracy=0.8125\n",
      "test accuracy = 0.6305000185966492\n",
      "loss at epoch 4:\n",
      "cost: 5.29137519340515, accuracy: 0.6443499999999988\n",
      "step=0, loss=4.735660552978516, accuracy=0.625\n",
      "test accuracy = 0.625\n",
      "step=100, loss=4.665027618408203, accuracy=0.75\n",
      "test accuracy = 0.6815000176429749\n",
      "step=200, loss=4.715360641479492, accuracy=0.625\n",
      "test accuracy = 0.6010000109672546\n",
      "step=300, loss=4.4570770263671875, accuracy=0.75\n",
      "test accuracy = 0.6930000185966492\n",
      "step=400, loss=4.595026969909668, accuracy=0.5625\n",
      "test accuracy = 0.6654999852180481\n",
      "step=500, loss=4.511553764343262, accuracy=0.625\n",
      "test accuracy = 0.6740000247955322\n",
      "step=600, loss=4.230012893676758, accuracy=0.75\n",
      "test accuracy = 0.690500020980835\n",
      "step=700, loss=4.256941318511963, accuracy=0.6875\n",
      "test accuracy = 0.6800000071525574\n",
      "step=800, loss=4.234133720397949, accuracy=0.5625\n",
      "test accuracy = 0.6819999814033508\n",
      "step=900, loss=4.107741355895996, accuracy=0.625\n",
      "test accuracy = 0.7135000228881836\n",
      "step=1000, loss=3.944420099258423, accuracy=0.75\n",
      "test accuracy = 0.6679999828338623\n",
      "step=1100, loss=4.408914089202881, accuracy=0.3125\n",
      "test accuracy = 0.6129999756813049\n",
      "step=1200, loss=3.785001516342163, accuracy=0.75\n",
      "test accuracy = 0.6650000214576721\n",
      "loss at epoch 5:\n",
      "cost: 4.313793719100946, accuracy: 0.6664999999999998\n",
      "step=0, loss=3.808502674102783, accuracy=0.625\n",
      "test accuracy = 0.5889999866485596\n",
      "step=100, loss=3.7562990188598633, accuracy=0.75\n",
      "test accuracy = 0.671999990940094\n",
      "step=200, loss=3.769170045852661, accuracy=0.625\n",
      "test accuracy = 0.6380000114440918\n",
      "step=300, loss=3.55079984664917, accuracy=0.75\n",
      "test accuracy = 0.6924999952316284\n",
      "step=400, loss=3.7785136699676514, accuracy=0.5\n",
      "test accuracy = 0.6424999833106995\n",
      "step=500, loss=3.652315855026245, accuracy=0.6875\n",
      "test accuracy = 0.7070000171661377\n",
      "step=600, loss=3.3220930099487305, accuracy=0.8125\n",
      "test accuracy = 0.6940000057220459\n",
      "step=700, loss=3.445504665374756, accuracy=0.6875\n",
      "test accuracy = 0.6775000095367432\n",
      "step=800, loss=3.415652275085449, accuracy=0.4375\n",
      "test accuracy = 0.6604999899864197\n",
      "step=900, loss=3.274435520172119, accuracy=0.625\n",
      "test accuracy = 0.7160000205039978\n",
      "step=1000, loss=3.1135315895080566, accuracy=0.6875\n",
      "test accuracy = 0.6940000057220459\n",
      "step=1100, loss=3.5052051544189453, accuracy=0.3125\n",
      "test accuracy = 0.6370000243186951\n",
      "step=1200, loss=2.989344358444214, accuracy=0.75\n",
      "test accuracy = 0.6859999895095825\n",
      "loss at epoch 6:\n",
      "cost: 3.439687343597416, accuracy: 0.6798000000000012\n",
      "step=0, loss=2.9977498054504395, accuracy=0.6875\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test accuracy = 0.6050000190734863\n",
      "step=100, loss=2.945345878601074, accuracy=0.75\n",
      "test accuracy = 0.6754999756813049\n",
      "step=200, loss=2.940401554107666, accuracy=0.625\n",
      "test accuracy = 0.6545000076293945\n",
      "step=300, loss=2.7718799114227295, accuracy=0.75\n",
      "test accuracy = 0.6955000162124634\n",
      "step=400, loss=2.983983039855957, accuracy=0.5\n",
      "test accuracy = 0.6535000205039978\n",
      "step=500, loss=2.893315315246582, accuracy=0.6875\n",
      "test accuracy = 0.7070000171661377\n",
      "step=600, loss=2.5638461112976074, accuracy=0.875\n",
      "test accuracy = 0.6974999904632568\n",
      "step=700, loss=2.7237792015075684, accuracy=0.625\n",
      "test accuracy = 0.6825000047683716\n",
      "step=800, loss=2.689957618713379, accuracy=0.5625\n",
      "test accuracy = 0.6654999852180481\n",
      "step=900, loss=2.563957691192627, accuracy=0.625\n",
      "test accuracy = 0.7135000228881836\n",
      "step=1000, loss=2.4590606689453125, accuracy=0.625\n",
      "test accuracy = 0.7110000252723694\n",
      "step=1100, loss=2.807037115097046, accuracy=0.375\n",
      "test accuracy = 0.6549999713897705\n",
      "step=1200, loss=2.326925277709961, accuracy=0.75\n",
      "test accuracy = 0.6974999904632568\n",
      "loss at epoch 7:\n",
      "cost: 2.701555637359618, accuracy: 0.6902500000000026\n",
      "step=0, loss=2.3396010398864746, accuracy=0.75\n",
      "test accuracy = 0.6240000128746033\n",
      "step=100, loss=2.308058738708496, accuracy=0.75\n",
      "test accuracy = 0.684499979019165\n",
      "step=200, loss=2.286947011947632, accuracy=0.625\n",
      "test accuracy = 0.6644999980926514\n",
      "step=300, loss=2.156280755996704, accuracy=0.6875\n",
      "test accuracy = 0.6955000162124634\n",
      "step=400, loss=2.3271572589874268, accuracy=0.625\n",
      "test accuracy = 0.6970000267028809\n",
      "step=500, loss=2.3088626861572266, accuracy=0.6875\n",
      "test accuracy = 0.715499997138977\n",
      "step=600, loss=1.9873887300491333, accuracy=0.875\n",
      "test accuracy = 0.6804999709129333\n",
      "step=700, loss=2.156555652618408, accuracy=0.6875\n",
      "test accuracy = 0.6880000233650208\n",
      "step=800, loss=2.122875690460205, accuracy=0.5625\n",
      "test accuracy = 0.6690000295639038\n",
      "step=900, loss=2.0144028663635254, accuracy=0.8125\n",
      "test accuracy = 0.7235000133514404\n",
      "step=1000, loss=1.927700161933899, accuracy=0.625\n",
      "test accuracy = 0.7149999737739563\n",
      "step=1100, loss=2.2776498794555664, accuracy=0.375\n",
      "test accuracy = 0.6635000109672546\n",
      "step=1200, loss=1.8318276405334473, accuracy=0.75\n",
      "test accuracy = 0.703499972820282\n",
      "loss at epoch 8:\n",
      "cost: 2.1245143675804115, accuracy: 0.6994000000000024\n",
      "step=0, loss=1.8569568395614624, accuracy=0.75\n",
      "test accuracy = 0.6625000238418579\n",
      "step=100, loss=1.8295111656188965, accuracy=0.75\n",
      "test accuracy = 0.6875\n",
      "step=200, loss=1.7937283515930176, accuracy=0.75\n",
      "test accuracy = 0.6725000143051147\n",
      "step=300, loss=1.6861233711242676, accuracy=0.6875\n",
      "test accuracy = 0.6995000243186951\n",
      "step=400, loss=1.8617019653320312, accuracy=0.6875\n",
      "test accuracy = 0.7124999761581421\n",
      "step=500, loss=1.900632381439209, accuracy=0.6875\n",
      "test accuracy = 0.7160000205039978\n",
      "step=600, loss=1.5511804819107056, accuracy=0.8125\n",
      "test accuracy = 0.6790000200271606\n",
      "step=700, loss=1.7259461879730225, accuracy=0.6875\n",
      "test accuracy = 0.6915000081062317\n",
      "step=800, loss=1.684300422668457, accuracy=0.625\n",
      "test accuracy = 0.6830000281333923\n",
      "step=900, loss=1.6240370273590088, accuracy=0.75\n",
      "test accuracy = 0.7264999747276306\n",
      "step=1000, loss=1.5548107624053955, accuracy=0.625\n",
      "test accuracy = 0.7195000052452087\n",
      "step=1100, loss=1.898789405822754, accuracy=0.375\n",
      "test accuracy = 0.6759999990463257\n",
      "step=1200, loss=1.4522898197174072, accuracy=0.75\n",
      "test accuracy = 0.7089999914169312\n",
      "loss at epoch 9:\n",
      "cost: 1.695623730850218, accuracy: 0.7077500000000025\n"
     ]
    }
   ],
   "source": [
    "batch_size = 64\n",
    "patch_size = 3\n",
    "depth = 32\n",
    "num_hidden = 64\n",
    "beta = 0.01\n",
    "x = tf.placeholder(tf.float32, [None, image_size, image_size, num_channels])\n",
    "y = tf.placeholder(tf.float32, [None, num_labels])\n",
    "\n",
    "# Parameters\n",
    "layer1_weights = tf.Variable(tf.truncated_normal([patch_size, patch_size, num_channels, depth], stddev=0.1))\n",
    "layer1_biases = tf.Variable(tf.zeros([depth]))\n",
    "layer2_weights = tf.Variable(tf.truncated_normal([patch_size, patch_size, depth, depth], stddev=0.1))\n",
    "layer2_biases = tf.Variable(tf.constant(1.0, shape=[depth]))\n",
    "layer3_weights = tf.Variable(tf.truncated_normal([image_size // 4 * image_size // 4 * depth, num_hidden], stddev=0.1))\n",
    "layer3_biases = tf.Variable(tf.constant(1.0, shape=[num_hidden]))\n",
    "layer4_weights = tf.Variable(tf.truncated_normal([num_hidden, num_labels], stddev=0.1))\n",
    "layer4_biases = tf.Variable(tf.constant(1.0, shape=[num_labels]))\n",
    "\n",
    "# Define Model\n",
    "def model(input_image):\n",
    "    conv1 = tf.nn.conv2d(input_image, layer1_weights, [1, 2, 2, 1], padding='SAME')\n",
    "    hidden1 = tf.nn.relu(conv1 + layer1_biases)\n",
    "    conv2 = tf.nn.conv2d(hidden1, layer2_weights, [1, 2, 2, 1], padding='SAME')\n",
    "    hidden2 = tf.nn.relu(conv2 + layer2_biases)\n",
    "    shape = hidden2.get_shape().as_list()\n",
    "    reshape = tf.reshape(hidden2, [-1, shape[1] * shape[2] * shape[3]])\n",
    "    hidden = tf.nn.relu(tf.matmul(reshape, layer3_weights) + layer3_biases)\n",
    "    return tf.matmul(hidden, layer4_weights) + layer4_biases\n",
    "\n",
    "# Training computation.\n",
    "logits = model(x)\n",
    "loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=y, logits=logits)\\\n",
    "                      +beta*tf.nn.l2_loss(layer1_weights)                                   #regularization\n",
    "                      +beta*tf.nn.l2_loss(layer2_weights)\n",
    "                      +beta*tf.nn.l2_loss(layer3_weights)\n",
    "                      +beta*tf.nn.l2_loss(layer4_weights)\n",
    "                     \n",
    "                     )\n",
    "\n",
    "# Optimizer\n",
    "optimizer = tf.train.AdamOptimizer(1e-4).minimize(loss)\n",
    "\n",
    "prediction = tf.equal(tf.argmax(logits, 1), tf.argmax(y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(prediction, tf.float32))\n",
    "\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    print('Initialized')\n",
    "    for epoch in range(total_epoch):\n",
    "        # calculate total batch within a epoch\n",
    "        num_steps =  len(train_data) // batch_size\n",
    "        avg_cost = 0 \n",
    "        avg_acc = 0.0\n",
    "        for step in range(num_steps):          \n",
    "            offset = (step * batch_size) % (len(train_labels) - batch_size)\n",
    "            batch_train_data_paths = train_data[offset:(offset + batch_size)]\n",
    "            batch_train_labels_paths = train_labels[offset:(offset + batch_size)]\n",
    "            batrch_train_data, batch_train_labels = load_batch_data(batch_train_data_paths, batch_train_labels_paths)\n",
    "            \n",
    "            feed_dict = { x: batrch_train_data, y: batch_train_labels}\n",
    "            _, l, train_accuracy_ = sess.run([optimizer, loss, accuracy], feed_dict=feed_dict)\n",
    "            \n",
    "            avg_cost = avg_cost + (l/num_steps)\n",
    "            avg_acc = avg_acc + (train_accuracy_/num_steps)\n",
    "            \n",
    "            if (step % 100 == 0):\n",
    "                saver.save(sess, \"train_model/model.ckpt\")\n",
    "                print('step={}, loss={}, accuracy={}'.format(step, l, train_accuracy_))\n",
    "                test_batch_data, test_batch_labels = load_batch_data(test_data[:2000], test_labels[:2000])\n",
    "                feed_dict = { x: test_batch_data, y: test_batch_labels}\n",
    "                test_accuracy_ = sess.run(accuracy, feed_dict=feed_dict)\n",
    "                print('test accuracy = {}'.format(test_accuracy_))\n",
    "\n",
    "            \n",
    "        print('loss at epoch {}:\\ncost: {}, accuracy: {}'.format(epoch, avg_cost, avg_acc))\n",
    "            \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# use pb file to store model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-5-4d13037ee588>:5: read_data_sets (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
      "WARNING:tensorflow:From /home/isaac/anaconda3/envs/tf-gpu/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:260: maybe_download (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please write your own downloading logic.\n",
      "WARNING:tensorflow:From /home/isaac/anaconda3/envs/tf-gpu/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:262: extract_images (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "WARNING:tensorflow:From /home/isaac/anaconda3/envs/tf-gpu/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:267: extract_labels (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From /home/isaac/anaconda3/envs/tf-gpu/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:110: dense_to_one_hot (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.one_hot on tensors.\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From /home/isaac/anaconda3/envs/tf-gpu/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:290: DataSet.__init__ (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
      "WARNING:tensorflow:From <ipython-input-5-4d13037ee588>:46: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See `tf.nn.softmax_cross_entropy_with_logits_v2`.\n",
      "\n",
      "0 accuracy= 0.05\n",
      "100 accuracy= 0.88\n",
      "200 accuracy= 0.92\n",
      "300 accuracy= 0.96\n",
      "400 accuracy= 0.99\n",
      "500 accuracy= 0.97\n",
      "600 accuracy= 0.94\n",
      "700 accuracy= 0.97\n",
      "800 accuracy= 0.94\n",
      "900 accuracy= 0.98\n",
      "INFO:tensorflow:Froze 8 variables.\n",
      "INFO:tensorflow:Converted 8 variables to const ops.\n",
      "WARNING:tensorflow:From <ipython-input-5-4d13037ee588>:71: FastGFile.__init__ (from tensorflow.python.platform.gfile) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.gfile.GFile.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "from tensorflow.python.framework import graph_util\n",
    "\n",
    "mnist = input_data.read_data_sets('MNIST_data', one_hot=True)\n",
    "\n",
    "\n",
    "#定义输入数据mnist图片大小28*28*1=784,None表示batch_size\n",
    "x = tf.placeholder(dtype=tf.float32,shape=[None,28*28],name=\"input\")\n",
    "#定义标签数据,mnist共10类\n",
    "y_ = tf.placeholder(dtype=tf.float32,shape=[None,10],name=\"y_\")\n",
    "#将数据调整为二维数据，w*H*c---> 28*28*1,-1表示N张\n",
    "image = tf.reshape(x,shape=[-1,28,28,1])\n",
    "\n",
    "#第一层，卷积核={5*5*1*32}，池化核={2*2*1,1*2*2*1}\n",
    "w1 = tf.Variable(initial_value=tf.random_normal(shape=[5,5,1,32],stddev=0.1,dtype=tf.float32,name=\"w1\"))\n",
    "b1= tf.Variable(initial_value=tf.zeros(shape=[32]))\n",
    "conv1 = tf.nn.conv2d(input=image,filter=w1,strides=[1,1,1,1],padding=\"SAME\",name=\"conv1\")\n",
    "relu1 = tf.nn.relu(tf.nn.bias_add(conv1,b1),name=\"relu1\")\n",
    "pool1 = tf.nn.max_pool(value=relu1,ksize=[1,2,2,1],strides=[1,2,2,1],padding=\"SAME\")\n",
    "#shape={None，14,14,32}\n",
    "#第二层，卷积核={5*5*32*64}，池化核={2*2*1,1*2*2*1}\n",
    "w2 = tf.Variable(initial_value=tf.random_normal(shape=[5,5,32,64],stddev=0.1,dtype=tf.float32,name=\"w2\"))\n",
    "b2 = tf.Variable(initial_value=tf.zeros(shape=[64]))\n",
    "conv2 = tf.nn.conv2d(input=pool1,filter=w2,strides=[1,1,1,1],padding=\"SAME\")\n",
    "relu2 = tf.nn.relu(tf.nn.bias_add(conv2,b2),name=\"relu2\")\n",
    "pool2 = tf.nn.max_pool(value=relu2,ksize=[1,2,2,1],strides=[1,2,2,1],padding=\"SAME\",name=\"pool2\")\n",
    "#shape={None，7,7,64}\n",
    "#FC1\n",
    "w3 = tf.Variable(initial_value=tf.random_normal(shape=[7*7*64,1024],stddev=0.1,dtype=tf.float32,name=\"w3\"))\n",
    "b3 = tf.Variable(initial_value=tf.zeros(shape=[1024]))\n",
    "#关键，进行reshape\n",
    "input3 = tf.reshape(pool2,shape=[-1,7*7*64],name=\"input3\")\n",
    "fc1 = tf.nn.relu(tf.nn.bias_add(value=tf.matmul(input3,w3),bias=b3),name=\"fc1\")\n",
    "#shape={None，1024}\n",
    "#FC2\n",
    "w4 = tf.Variable(initial_value=tf.random_normal(shape=[1024,10],stddev=0.1,dtype=tf.float32,name=\"w4\"))\n",
    "b4 = tf.Variable(initial_value=tf.zeros(shape=[10]))\n",
    "fc2 = tf.nn.bias_add(value=tf.matmul(fc1,w4),bias=b4)\n",
    "#shape={None，10}\n",
    "#定义交叉熵损失\n",
    "# 使用softmax将NN计算输出值表示为概率\n",
    "y = tf.nn.softmax(fc2,name=\"out\")\n",
    "\n",
    "# 定义交叉熵损失函数\n",
    "cross_entropy = tf.nn.softmax_cross_entropy_with_logits(logits=fc2,labels=y_)\n",
    "loss = tf.reduce_mean(cross_entropy)\n",
    "#定义solver\n",
    "train = tf.train.AdamOptimizer(learning_rate=0.0001).minimize(loss=loss)\n",
    "\n",
    "#定义正确值,判断二者下标index是否相等\n",
    "correct_predict = tf.equal(tf.argmax(y,1),tf.argmax(y_,1))\n",
    "#定义如何计算准确率\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_predict,dtype=tf.float32),name=\"accuracy\")\n",
    "\n",
    "\n",
    "#训练NN\n",
    "with tf.Session() as session:\n",
    "    session.run(tf.global_variables_initializer())\n",
    "    for i in range(0,1000):\n",
    "        xs, ys = mnist.train.next_batch(100)\n",
    "        session.run(fetches=train,feed_dict={x:xs,y_:ys})\n",
    "        if i%100 == 0:\n",
    "            train_accuracy = session.run(fetches=accuracy,feed_dict={x:xs,y_:ys})\n",
    "            print(i,\"accuracy=\",train_accuracy)\n",
    "    #训练完成后，将网络中的权值转化为常量，形成常量graph\n",
    "    constant_graph = graph_util.convert_variables_to_constants(sess=session,\n",
    "                                                            input_graph_def=session.graph_def,\n",
    "                                                            output_node_names=['out'])\n",
    "    #将带权值的graph序列化，写成pb文件存储起来\n",
    "    with tf.gfile.FastGFile(\"lenet.pb\", mode='wb') as f:\n",
    "        f.write(constant_graph.SerializeToString())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n",
      "pre_label= [6]\n",
      "true label: 6\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "import numpy as np\n",
    "\n",
    "mnist = input_data.read_data_sets('MNIST_data',one_hot=True)\n",
    "pb_path = \"lenet.pb\"\n",
    "\n",
    "#导入pb文件到graph中\n",
    "with tf.gfile.FastGFile(pb_path,'rb') as f:\n",
    "    # 复制定义好的计算图到新的图中，先创建一个空的图.\n",
    "    graph_def = tf.GraphDef()\n",
    "    # 加载proto-buf中的模型\n",
    "    graph_def.ParseFromString(f.read())\n",
    "    # 最后复制pre-def图的到默认图中.\n",
    "    _ = tf.import_graph_def(graph_def, name='')\n",
    "    \n",
    "with tf.Session() as session:\n",
    "    #获取输入tensor\n",
    "    session.run(tf.global_variables_initializer())\n",
    "    input_x = tf.get_default_graph().get_tensor_by_name(\"input:0\")\n",
    "    #获取预测tensor\n",
    "    output = tf.get_default_graph().get_tensor_by_name(\"out:0\")\n",
    "    #取第100张图片测试\n",
    "    one_image = np.reshape(mnist.test.images[100], [-1, 784])\n",
    "    #将测试图片传入nn中，做inference\n",
    "    out = session.run(output,feed_dict={input_x:one_image})\n",
    "    pre_label = np.argmax(out,1)\n",
    "    print(\"pre_label=\",pre_label)\n",
    "    print('true label:', np.argmax(mnist.test.labels[100],0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
